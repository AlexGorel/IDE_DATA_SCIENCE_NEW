{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTHON-12. Продвинутые методы работы с данными в Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "melb_df = pd.read_csv('data_12/melb_data_fe.csv')\n",
    "melb_df.head()\n",
    "#melb_df.info()\n",
    "melb_df['Date'] = pd.to_datetime(melb_df['Date'])\n",
    "#melb_df.info()\n",
    "#display(melb_df['Date'])\n",
    "\n",
    "quarter_sold = melb_df['Date'].dt.quarter\n",
    "#print(quarter_sold.value_counts()) #Задание 1.1\n",
    "\n",
    "cols_to_exclude = ['Date', 'Rooms', 'Bedroom', 'Bathroom', 'Car'] # список столбцов, которые мы не берём во внимание\n",
    "max_unique_count = 150 # задаём максимальное число уникальных категорий\n",
    "for col in melb_df.columns: # цикл по именам столбцов\n",
    "    if melb_df[col].nunique() < max_unique_count and col not in cols_to_exclude: # проверяем условие\n",
    "        melb_df[col] = melb_df[col].astype('category') # преобразуем тип столбца\n",
    "display(melb_df.info())  # Задание 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Сортировка данных в DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "melb_df = pd.read_csv('data_12/melb_data_fe.csv')\n",
    "melb_df.head()\n",
    "#melb_df.info()\n",
    "melb_df['Date'] = pd.to_datetime(melb_df['Date'])\n",
    "#melb_df.info()\n",
    "#display(melb_df['Date'])\n",
    "\n",
    "quarter_sold = melb_df['Date'].dt.quarter\n",
    "#print(quarter_sold.value_counts()) #Задание 1.1\n",
    "\n",
    "cols_to_exclude = ['Date', 'Rooms', 'Bedroom', 'Bathroom', 'Car'] # список столбцов, которые мы не берём во внимание\n",
    "max_unique_count = 150 # задаём максимальное число уникальных категорий\n",
    "for col in melb_df.columns: # цикл по именам столбцов\n",
    "    if melb_df[col].nunique() < max_unique_count and col not in cols_to_exclude: # проверяем условие\n",
    "        melb_df[col] = melb_df[col].astype('category') # преобразуем тип столбца\n",
    "#display(melb_df.info())  # Задание 1.2\n",
    "melb_df.sort_values(by='Price').head(10)#Отсортируем таблицу по возрастанию цены объектов недвижимости (Price):\n",
    "melb_df.sort_values(by='Date', ascending=False)# отсортируем таблицу по убыванию (от самой последней до самой первой) даты продажи объекта (Date). Для этого выставим параметр ascending на False\n",
    "melb_df.sort_values(by=['Distance', 'Price']).loc[::10, ['Distance', 'Price']]#отсортируем таблицу сначала по возрастанию расстояния от центра города (Distance), а затем — по возрастанию цены объекта (Price). Для того чтобы вывод был более наглядным, выделим каждую десятую строку из столбцов Distance и Price результирующей таблицы\n",
    "#melb_df.sort_values(by=['Distance', 'Price'])\n",
    "\n",
    "mask1 = melb_df['AreaRatio'] < -0.8\n",
    "mask2 = melb_df['Type'] == 'townhouse'\n",
    "mask3 = melb_df['SellerG'] == 'McGrath'\n",
    "melb_df[mask1 & mask2 & mask3].sort_values(by=['Date', 'AreaRatio'],ascending=[True, False],ignore_index=True).loc[:, ['Date', 'AreaRatio']]\n",
    "\n",
    "melb_df.sort_values(by='AreaRatio',ignore_index=True, ascending=False).loc[1558, 'BuildingArea'] #Задание 2.2\n",
    "\n",
    "mask111 = melb_df['Type'] == 'townhouse'\n",
    "mask222 = melb_df['Rooms'] > 2\n",
    "#int(melb_df[mask111&mask222].sort_values(by=['Rooms', 'MeanRoomsSquare'],ascending=[True, False],ignore_index=True).loc[18, 'Price'])\n",
    "int(melb_df[(melb_df['Type'] == 'townhouse') & (melb_df['Rooms'] > 2)].sort_values(by=['Rooms', 'MeanRoomsSquare'],ascending=[True, False],ignore_index=True).loc[18, 'Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Группировка данных в DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "melb_df = pd.read_csv('data_12/melb_data_fe.csv')\n",
    "melb_df.head()\n",
    "#melb_df.info()\n",
    "melb_df['Date'] = pd.to_datetime(melb_df['Date'])\n",
    "melb_df.groupby(by='Type')\n",
    "melb_df.groupby('Type')['Price'].mean() #сравним средние цены на объекты в зависимости от их типа\n",
    "melb_df.groupby('Regionname')['Distance'].min().sort_values(ascending=False)# найдём минимальное значение расстояния от центра города до объекта в зависимости от его региона. Результат отсортируем по убыванию расстояния\n",
    "melb_df.groupby('Regionname')['Distance'].min().sort_values()\n",
    "\n",
    "melb_df.groupby('MonthSale')['Price'].agg(['count', 'mean', 'max']).sort_values(by='count', ascending=False) #построим таблицу для анализа продаж по месяцам. Для этого найдём количество продаж, а также среднее и максимальное значения цен объектов недвижимости (Price), сгруппированных по номеру месяца продажи (MonthSale). Результат отсортируем по количеству продаж в порядке убывания\n",
    "melb_df.groupby('MonthSale')['Price'].agg('describe').sort_values(by='count', ascending=False) #Если вам нужна полная информация обо всех основных статистических характеристиках внутри каждой группы, вы можете воспользоваться методом agg(), передав в качестве его параметра строку 'describe'\n",
    "\n",
    "melb_df.groupby('Regionname')['SellerG'].agg(['nunique', set])\n",
    "melb_df.groupby('Regionname')['SellerG'].agg(['nunique']).sort_values(by='nunique', ascending=False)\n",
    "\n",
    "melb_df.groupby('Rooms')['Price'].mean().sort_values(ascending=False)#ЗАДАНИЕ 3.1\n",
    "\n",
    "melb_df.groupby('Regionname')['Lattitude'].std().sort_values()#ЗАДАНИЕ 3.2\n",
    "\n",
    "mask555 = ('2017-05-01' <= melb_df['Date']) & (melb_df['Date']<= '2017-09-01') ##ЗАДАНИЕ 3.2\n",
    "melb_df[mask555].groupby('SellerG')['Price'].sum().sort_values()\n",
    "melb_df[('2017-05-01' <= melb_df['Date']) & (melb_df['Date']<= '2017-09-01')].groupby('SellerG')['Price'].sum().sort_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Сводные таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "melb_df = pd.read_csv('data_12/melb_data_fe.csv')\n",
    "melb_df.head()\n",
    "#melb_df.info()\n",
    "melb_df['Date'] = pd.to_datetime(melb_df['Date'])\n",
    "#print(melb_df.groupby('Rooms')[['Price', 'BuildingArea']].median().sort_index())\n",
    "#print(melb_df.groupby('Rooms')[['Price', 'BuildingArea']].median().sort_values(by='Rooms'))\n",
    "#print(melb_df.groupby('Rooms')[['Price', 'BuildingArea']].median()) #строить таблицу, которая показывает зависимость медианной цены и площади здания от числа комнат\n",
    "#print(melb_df.groupby(['Rooms', 'Type'])['Price'].mean()) #можно построить таблицу, в которой мы будем учитывать не только число комнат, но и тип здания (Type). Для этого в параметрах метода groupby() укажем список из нескольких интересующих нас столбцов\n",
    "#print(melb_df.groupby(['Rooms', 'Type'])['Price'].mean().unstack())\n",
    "\n",
    "melb_df.pivot_table(values='Price',index='Rooms',columns='Type',fill_value=0).round()\n",
    "\n",
    "melb_df.pivot_table(values='Price',index='Regionname',columns='Weekend',aggfunc='count') # давайте проанализируем продажи в каждом из регионов в зависимости от того, будний был день или выходной. Для этого построим сводную таблицу, в которой строками будут являться названия регионов (Regionname), а в столбцах будет располагаться наш «признак-мигалка» выходного дня (Weekend), который равен 1, если день был выходным, и 0 — в противном случае. В качестве значений сводной таблицы возьмём количество продаж\n",
    "\n",
    "melb_df.pivot_table(values='Landsize',index='Regionname',columns='Type',aggfunc=['median', 'mean'],fill_value=0)# найдём, как зависит средняя и медианная площадь участка (Landsize) от типа объекта (Type) и его региона (Regionname). Чтобы посмотреть несколько статистических параметров, нужно передать в аргумент aggfunc список из агрегирующих функций. Построим такую сводную таблицу, где пропущенные значения заменим на 0\n",
    "\n",
    "melb_df.pivot_table(values='Price',index=['Method','Type'],columns='Regionname',aggfunc='median',fill_value=0)#построим таблицу, в которой по индексам будут располагаться признаки метода продажи (Method) и типа объекта (Type), по столбцам — наименование региона (Regionname), а на пересечении строк и столбцов будет стоять медианная цена объекта (Price)\n",
    "\n",
    "pivot=melb_df.pivot_table(values='Landsize',index='Regionname',columns='Type',aggfunc=['median', 'mean'],fill_value=0)\n",
    "pivot.columns\n",
    "#display(pivot['mean']['unit'])\n",
    "\n",
    "mask = pivot['mean']['house'] < pivot['median']['house'] #нам нужны регионы, в которых средняя площадь здания для домов типа house меньше их медианной площади, то мы можем найти их следующим образом\n",
    "filtered_pivot = pivot[mask] \n",
    "#display(filtered_pivot)\n",
    "#print(list(filtered_pivot.index))\n",
    "\n",
    "#print(melb_df.pivot_table(values='BuildingArea',index='Rooms',columns='Type',aggfunc='median',fill_value=0)) #Задание 4.2\n",
    "#print(melb_df.pivot_table(values='BuildingArea',index='Type',columns='Rooms',aggfunc='median',fill_value=0)) #  или так (ответ)\n",
    "\n",
    "pivot=melb_df.pivot_table(values='Price',index='SellerG',columns='Type',aggfunc='median',fill_value=0) #ЗАДАНИЕ 4.3\n",
    "max_unit_price = pivot['unit'].max()\n",
    "print(pivot     [pivot['unit'] == max_unit_price]  .index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Объединение DataFrame: знакомимся с новыми данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings1 = pd.read_csv('data_12/ratings1.csv')\n",
    "ratings2 = pd.read_csv('data_12/ratings2.csv')\n",
    "dates = pd.read_csv('data_12/dates.csv')\n",
    "movies = pd.read_csv('data_12/movies.csv')\n",
    "#print(ratings1.head())\n",
    "#print(ratings2.head())\n",
    "#print(dates.head())\n",
    "#print(movies.head())\n",
    "#print(movies.shape[0]) #Задание 5.2\n",
    "#print(ratings1.nunique()[0]) #Задание 5.2\n",
    "#print(dates.value_counts().max())\n",
    "#print(dates.info())\n",
    "#print(dates.mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Объединение DataFrame: concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings1 = pd.read_csv('data_12/ratings1.csv')\n",
    "ratings2 = pd.read_csv('data_12/ratings2.csv')\n",
    "dates = pd.read_csv('data_12/dates.csv')\n",
    "movies = pd.read_csv('data_12/movies.csv')\n",
    "ratings = pd.concat([ratings1, ratings2],ignore_index=True) #склеим  ratings1 и ratings2 по строкам, так как они имеют одинаковую структуру столбцов. Для этого передадим их списком в функцию concat(). Помним, что параметр axis по умолчанию равен 0, объединение происходит по строкам, поэтому не трогаем ег\n",
    "#display(ratings)\n",
    "\n",
    "#display(ratings1.tail(1))\n",
    "#display(ratings2.head(1))\n",
    "\n",
    "ratings = ratings.drop_duplicates(ignore_index=True) #Чтобы очистить таблицу от дублей, мы можем воспользоваться методом DataFrame drop_duplicates(), который удаляет повторяющиеся строки в таблице. Не забываем обновить индексы после удаления дублей, выставив параметр ignore_index в методе drop_duplicates() на значение True\n",
    "#print('Число строк в таблице ratings: ', ratings.shape[0])\n",
    "#print('Число строк в таблице dates: ', dates.shape[0])\n",
    "#print(ratings.shape[0] == dates.shape[0])\n",
    "\n",
    "ratings_dates = pd.concat([ratings, dates], axis=1)#можем добавить к нашей таблице с оценками даты их выставления. Для этого конкатенируем таблицы ratings и dates по столбцам\n",
    "display(ratings_dates.tail(7))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Role    Name   Surname\n",
      "0   Admin  Pankaj   Sobolev\n",
      "1  Editor    Lisa  Krasnova\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"Name\": [\"Pankaj\", \"Lisa\"], \"Surname\": [\"Sobolev\", \"Krasnova\"]})\n",
    "df2 = pd.DataFrame({\"Role\": [\"Admin\", \"Editor\"]})\n",
    "#df = pd.concat([df1, df2], axis=1)\n",
    "df = pd.concat([df2, df1], axis=1)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Объединение DataFrame: join, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings1 = pd.read_csv('data_12/ratings1.csv')\n",
    "ratings2 = pd.read_csv('data_12/ratings2.csv')\n",
    "dates = pd.read_csv('data_12/dates.csv')\n",
    "movies = pd.read_csv('data_12/movies.csv')\n",
    "ratings = pd.concat([ratings1, ratings2],ignore_index=True) #склеим  ratings1 и ratings2 по строкам, так как они имеют одинаковую структуру столбцов. Для этого передадим их списком в функцию concat(). Помним, что параметр axis по умолчанию равен 0, объединение происходит по строкам, поэтому не трогаем ег\n",
    "#display(ratings)\n",
    "\n",
    "#display(ratings1.tail(1))\n",
    "#display(ratings2.head(1))\n",
    "\n",
    "ratings = ratings.drop_duplicates(ignore_index=True) #Чтобы очистить таблицу от дублей, мы можем воспользоваться методом DataFrame drop_duplicates(), который удаляет повторяющиеся строки в таблице. Не забываем обновить индексы после удаления дублей, выставив параметр ignore_index в методе drop_duplicates() на значение True\n",
    "#print('Число строк в таблице ratings: ', ratings.shape[0])\n",
    "#print('Число строк в таблице dates: ', dates.shape[0])\n",
    "#print(ratings.shape[0] == dates.shape[0])\n",
    "\n",
    "ratings_dates = pd.concat([ratings, dates], axis=1)#можем добавить к нашей таблице с оценками даты их выставления. Для этого конкатенируем таблицы ratings и dates по столбцам\n",
    "#display(ratings_dates.tail(7))\n",
    "\n",
    "#joined_false = ratings_dates.join(movies,rsuffix='_right',how='left')\n",
    "#display(joined_false)\n",
    "\n",
    "#joined = ratings_dates.join(movies.set_index('movieId'),on='movieId',how='left')\n",
    "#display(joined)\n",
    "\n",
    "merged = ratings_dates.merge(movies,on='movieId',how='left')\n",
    "display(merged.head())\n",
    "print('Число строк в таблице ratings_dates: ', ratings_dates.shape[0])\n",
    "print('Число строк в таблице merged: ', merged.shape[0])\n",
    "print(ratings_dates.shape[0] == merged.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings1 = pd.read_csv('data_12/ratings1.csv')\n",
    "ratings2 = pd.read_csv('data_12/ratings2.csv')\n",
    "dates = pd.read_csv('data_12/dates.csv')\n",
    "movies = pd.read_csv('data_12/movies.csv')\n",
    "merge_ratings = ratings1.merge(ratings2, how='outer')\n",
    "print('Число строк в таблице merge_ratings: ', merge_ratings.shape[0])\n",
    "display(merge_ratings)\n",
    "# Число строк в таблице merge_ratings: 100836"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.DataFrame({'Value': [100, 45, 80],'Group': [1, 4, 5]},index = ['I0', 'I1', 'I2'])\n",
    "data_2 = pd.DataFrame({'Company': ['Google', 'Amazon', 'Facebook'],'Add': ['S0', 'S1', 'S7']},index = ['I0', 'I1', 'I3'])\n",
    "display(data_1)\n",
    "display(data_2)\n",
    "#display(data_1.join(data_2, how='inner'))\n",
    "#display(data_1.join(data_2, how='outer'))\n",
    "#display(data_1.join(data_2, how='left'))\n",
    "#display(data_1.join(data_2, how='right'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'A': ['a', 'b', 'c'], 'B': [103, 214, 124], 'C': [1, 4, 2]})\n",
    "b = pd.DataFrame({'V': ['d', 'b', 'c'], 'U': [1393.7, 9382.2, 1904.5], 'C': [1, 3, 2]})\n",
    "print(a)\n",
    "print(b)\n",
    "display(a.merge(b, how='right'))\n",
    "display(a.merge(b, how='left',on='C'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 7.5 (External resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "items_df = pd.DataFrame({\n",
    "    'item_id': [417283, 849734, 132223, 573943, 19475, 3294095, 382043, 302948, 100132, 312394],\n",
    "    'vendor': ['Samsung', 'LG', 'Apple', 'Apple', 'LG', 'Apple', 'Samsung', 'Samsung', 'LG', 'ZTE'],\n",
    "    'stock_count': [54, 33, 122, 18, 102, 43, 77, 143, 60, 19]\n",
    "})\n",
    "purchase_df = pd.DataFrame({\n",
    "    'purchase_id': [101, 101, 101, 112, 121, 145, 145, 145, 145, 221],\n",
    "    'item_id': [417283, 849734, 132223, 573943, 19475, 3294095, 382043, 302948, 103845, 100132],\n",
    "    'price': [13900, 5330, 38200, 49990, 9890, 33000, 67500, 34500, 89900, 11400]\n",
    "})\n",
    "merged=items_df.merge(purchase_df,on='item_id',how='inner')\n",
    "income=(merged['price']*merged['stock_count']).sum()\n",
    "print(merged)\n",
    "print(income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Закрепление знаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings_movies_data = pd.read_csv('data_12/ratings_movies.csv')\n",
    "ratings_movies_data_df = ratings_movies_data.copy()\n",
    "\n",
    "#print(ratings_movies_data_df.head())\n",
    "#print(ratings_movies_data_df.shape) \n",
    "#print(ratings_movies_data_df.nunique())\n",
    "#print(ratings_movies_data_df.value_counts())\n",
    "#print(ratings_movies_data_df.info())\n",
    "#библиотека для регулярных выражений\n",
    "import re \n",
    "def get_year_release(arg):\n",
    "    #находим все слова по шаблону \"(DDDD)\"\n",
    "    candidates = re.findall(r'\\(\\d{4}\\)', arg) \n",
    "    # проверяем число вхождений\n",
    "    if len(candidates) > 0:\n",
    "        #если число вхождений больше 0,\n",
    "\t#очищаем строку от знаков \"(\" и \")\"\n",
    "        year = candidates[0].replace('(', '')\n",
    "        year = year.replace(')', '')\n",
    "        return int(year)\n",
    "    else:\n",
    "        #если год не указан, возвращаем None\n",
    "        return None\n",
    "ratings_movies_data_df['year_release'] = ratings_movies_data_df['title'].apply(get_year_release) #Задание 8.1\n",
    "#print(ratings_movies_data_df.info())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
